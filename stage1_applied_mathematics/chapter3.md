# 3章: 情報理論
## 自己情報量
 $$ I(x)=-log(P(x)) = log(W(x)) $$

※Pは事象xが起きる確率、Wは事象xが起きる数を表す

- 底が2の時、単位はbit
- 底がeの時、単位はnat（naturalのnat）

## シャノンエントロピー
 $$ H(x)=E(I(x))=-E(log(P(x))) = -\displaystyle \sum(P(x)log(P(x))) $$

- 自己情報量の期待値

## KLダイバージェンス
$$ D_{KL}(P||Q) = \mathbb E_{x～P}[logP(x)/Q(x)] = \mathbb E_{x～P}[logP(x) - logQ(x)] $$
- KLはカルバック・ライブラー
- ダイバージェンスは距離のようなもの。P, Qの性質の違い。
- 同じ事象、確率変数における異なる確率分布P,Qの違いを表す
- シャノンエントロピーと式が似ている
- マイナス値は取らない。距離っぽく扱える

## 交差エントロピー
 $$
 H(P,Q) = H(P) + D_{KL}(P||Q) \\
 H(P,Q) = \mathbb E_{x～P}logQ(x)
 $$
- KLダイバージェンスの一部分を取り出したもの
- Qについての自己情報量をPの分布で平均している
